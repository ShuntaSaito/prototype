{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import time\n",
    "import eli5\n",
    "\n",
    "import mpl_finance as mpf\n",
    "from matplotlib import ticker\n",
    "import matplotlib.dates as mdates\n",
    "from pyti.exponential_moving_average import exponential_moving_average as ema\n",
    "from pyti.moving_average_convergence_divergence import moving_average_convergence_divergence as macd\n",
    "from pyti.simple_moving_average import simple_moving_average as sma\n",
    "import talib as ta\n",
    "from tslearn.clustering import KShape, TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "gc.enable()\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = ['USD_JPY']\n",
    "instruments = ['EUR_JPY', 'GBP_JPY', 'GBP_USD', 'EUR_USD', 'SGD_JPY', 'JP225_USD', 'US30_USD','NAS100_USD','SPX500_USD',\n",
    "               'EU50_EUR','DE30_EUR','AU200_AUD','USB05Y_USD','USB10Y_USD','XAU_EUR','XAG_GBP', 'XAU_XAG', 'WHEAT_USD']\n",
    "granularities = ['M5']\n",
    "where = ['2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"C:/Users/goomo/Desktop/FX_ProtoType/DB/{}.db\".format(main[0])\n",
    "conn = sqlite3.connect(dbname)\n",
    "cur = conn.cursor()\n",
    "df =  pd.read_sql('SELECT * FROM {} WHERE Date > {} '.format(granularities[0], where[0]), conn)\n",
    "cur.close()\n",
    "conn.close()\n",
    "df = df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, instrument in enumerate(instruments):\n",
    "    dbname = \"C:/Users/goomo/Desktop/FX_ProtoType/DB/{}.db\".format(instrument)\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    cur = conn.cursor()\n",
    "    _df = pd.read_sql('SELECT * FROM {} WHERE Date > {} '.format(granularities[0], where[0]), conn)\n",
    "    _df.columns = [\"index\", \"Date\", \"{}_Open\".format(instrument), \n",
    "                                    \"{}_High\".format(instrument), \n",
    "                                    \"{}_Low\".format(instrument), \n",
    "                                    \"{}_Close\".format(instrument),\n",
    "                                    \"{}_Volume\".format(instrument)]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    _df = _df.drop(columns='index')\n",
    "    _df['Date'] = pd.to_datetime(_df['Date'])\n",
    "    \n",
    "    if instrument  == 'USD_JPY':\n",
    "        df_USDJPY = _df\n",
    "    elif instrument == 'EUR_JPY':\n",
    "        df_EURJPY = _df\n",
    "    elif instrument == 'GBP_JPY':\n",
    "        df_GBPJPY = _df\n",
    "    elif instrument == 'GBP_USD':\n",
    "        df_GBPUSD = _df\n",
    "    elif instrument == 'EUR_USD':\n",
    "        df_EURUSD = _df\n",
    "    elif instrument == 'SGD_JPY':\n",
    "        df_SGDJPY = _df\n",
    "    elif instrument == 'JP225_USD':\n",
    "        df_JP225  = _df\n",
    "    elif instrument == 'US30_USD':\n",
    "        df_US30   = _df\n",
    "    elif instrument == 'NAS100_USD':\n",
    "        df_NAS100 = _df\n",
    "    elif instrument == 'SPX500_USD':\n",
    "        df_SPX500 = _df\n",
    "    elif instrument == 'EU50_EUR':\n",
    "        df_EU50   = _df\n",
    "    elif instrument == 'DE30_EUR':\n",
    "        df_DE30   = _df\n",
    "    elif instrument == 'AU200_AUD':\n",
    "        df_AU200  = _df\n",
    "    elif instrument == 'USB05Y_USD':\n",
    "        df_USB05Y = _df\n",
    "    elif instrument == 'USB10Y_USD':\n",
    "        df_USB10Y = _df\n",
    "    elif instrument == 'XAU_EUR':\n",
    "        df_XAUEUR = _df\n",
    "    elif instrument == 'XAG_GBP':\n",
    "        df_XAGGBP = _df\n",
    "    elif instrument == 'XAU_XAG':\n",
    "        df_XAUXAG = _df\n",
    "    elif instrument == 'WHEAT_USD':\n",
    "        df_WHEAT = _df\n",
    "\n",
    "del _df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01 22:00:00</td>\n",
       "      <td>105.246</td>\n",
       "      <td>105.331</td>\n",
       "      <td>105.242</td>\n",
       "      <td>105.323</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01 22:05:00</td>\n",
       "      <td>105.324</td>\n",
       "      <td>105.372</td>\n",
       "      <td>105.321</td>\n",
       "      <td>105.365</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date     Open     High      Low    Close  Volume\n",
       "0  2014-01-01 22:00:00  105.246  105.331  105.242  105.323     142\n",
       "1  2014-01-01 22:05:00  105.324  105.372  105.321  105.365     116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 求めるデータを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df['Date'].str.split(\" \", expand=True)\n",
    "_df = pd.to_datetime(_df[0], format='%Y/%m/%d')\n",
    "Date_key = pd.DataFrame({'DATE':_df})\n",
    "df = pd.concat([df, Date_key],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AFTER_2H'] = df['Close'].shift(-36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_2H  = []\n",
    "for i in range(len(df)):\n",
    "\n",
    "    DIFF_2H.append((df['AFTER_2H'][i] - df['Close'][i]))\n",
    "\n",
    "    \n",
    "DIFF_2H = pd.DataFrame({'DIFF_AFTER_2H':DIFF_2H})\n",
    "\n",
    "df = pd.concat([df, DIFF_2H],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Profit_2Classification(df, _DiffDay, window_len):    \n",
    "    profit   = []\n",
    "    for i in range(len(df) - window_len):\n",
    "        if _DiffDay[i] > 0:\n",
    "            profit.append('1')\n",
    "        else:\n",
    "            profit.append('0')\n",
    "              \n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = 24\n",
    "profit = Profit_2Classification(df, df['DIFF_AFTER_2H'], windows)\n",
    "PROFIT_2H  = pd.DataFrame({'PROFIT_2H':profit})\n",
    "df = pd.concat([df, PROFIT_2H],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DATE</th>\n",
       "      <th>AFTER_2H</th>\n",
       "      <th>DIFF_AFTER_2H</th>\n",
       "      <th>PROFIT_2H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390135</th>\n",
       "      <td>2019-03-29 20:35:00</td>\n",
       "      <td>110.822</td>\n",
       "      <td>110.822</td>\n",
       "      <td>110.806</td>\n",
       "      <td>110.810</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390136</th>\n",
       "      <td>2019-03-29 20:40:00</td>\n",
       "      <td>110.813</td>\n",
       "      <td>110.820</td>\n",
       "      <td>110.813</td>\n",
       "      <td>110.818</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390137</th>\n",
       "      <td>2019-03-29 20:45:00</td>\n",
       "      <td>110.814</td>\n",
       "      <td>110.822</td>\n",
       "      <td>110.813</td>\n",
       "      <td>110.818</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390138</th>\n",
       "      <td>2019-03-29 20:50:00</td>\n",
       "      <td>110.820</td>\n",
       "      <td>110.828</td>\n",
       "      <td>110.812</td>\n",
       "      <td>110.826</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390139</th>\n",
       "      <td>2019-03-29 20:55:00</td>\n",
       "      <td>110.824</td>\n",
       "      <td>110.858</td>\n",
       "      <td>110.818</td>\n",
       "      <td>110.850</td>\n",
       "      <td>34</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date     Open     High      Low    Close  Volume  \\\n",
       "390135  2019-03-29 20:35:00  110.822  110.822  110.806  110.810      18   \n",
       "390136  2019-03-29 20:40:00  110.813  110.820  110.813  110.818      13   \n",
       "390137  2019-03-29 20:45:00  110.814  110.822  110.813  110.818      12   \n",
       "390138  2019-03-29 20:50:00  110.820  110.828  110.812  110.826      33   \n",
       "390139  2019-03-29 20:55:00  110.824  110.858  110.818  110.850      34   \n",
       "\n",
       "             DATE  AFTER_2H  DIFF_AFTER_2H PROFIT_2H  \n",
       "390135 2019-03-29       NaN            NaN       NaN  \n",
       "390136 2019-03-29       NaN            NaN       NaN  \n",
       "390137 2019-03-29       NaN            NaN       NaN  \n",
       "390138 2019-03-29       NaN            NaN       NaN  \n",
       "390139 2019-03-29       NaN            NaN       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DATE</th>\n",
       "      <th>AFTER_2H</th>\n",
       "      <th>DIFF_AFTER_2H</th>\n",
       "      <th>PROFIT_2H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390111</th>\n",
       "      <td>2019-03-29 18:35:00</td>\n",
       "      <td>110.820</td>\n",
       "      <td>110.820</td>\n",
       "      <td>110.813</td>\n",
       "      <td>110.818</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390112</th>\n",
       "      <td>2019-03-29 18:40:00</td>\n",
       "      <td>110.821</td>\n",
       "      <td>110.824</td>\n",
       "      <td>110.800</td>\n",
       "      <td>110.802</td>\n",
       "      <td>35</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390113</th>\n",
       "      <td>2019-03-29 18:45:00</td>\n",
       "      <td>110.804</td>\n",
       "      <td>110.824</td>\n",
       "      <td>110.802</td>\n",
       "      <td>110.818</td>\n",
       "      <td>32</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390114</th>\n",
       "      <td>2019-03-29 18:50:00</td>\n",
       "      <td>110.815</td>\n",
       "      <td>110.858</td>\n",
       "      <td>110.815</td>\n",
       "      <td>110.838</td>\n",
       "      <td>66</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390115</th>\n",
       "      <td>2019-03-29 18:55:00</td>\n",
       "      <td>110.840</td>\n",
       "      <td>110.856</td>\n",
       "      <td>110.806</td>\n",
       "      <td>110.812</td>\n",
       "      <td>132</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date     Open     High      Low    Close  Volume  \\\n",
       "390111  2019-03-29 18:35:00  110.820  110.820  110.813  110.818      10   \n",
       "390112  2019-03-29 18:40:00  110.821  110.824  110.800  110.802      35   \n",
       "390113  2019-03-29 18:45:00  110.804  110.824  110.802  110.818      32   \n",
       "390114  2019-03-29 18:50:00  110.815  110.858  110.815  110.838      66   \n",
       "390115  2019-03-29 18:55:00  110.840  110.856  110.806  110.812     132   \n",
       "\n",
       "             DATE  AFTER_2H  DIFF_AFTER_2H PROFIT_2H  \n",
       "390111 2019-03-29       NaN            NaN         0  \n",
       "390112 2019-03-29       NaN            NaN         0  \n",
       "390113 2019-03-29       NaN            NaN         0  \n",
       "390114 2019-03-29       NaN            NaN         0  \n",
       "390115 2019-03-29       NaN            NaN         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:-24]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 階差・対数収益率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log_Diff(ts,window):    \n",
    "    logDiff = np.log(ts) - np.log(ts.shift(window))     \n",
    "    return logDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['15m_O'] = df['Open'].diff(3)\n",
    "df['30m_O'] = df['Open'].diff(6)\n",
    "df['1H_O']  = df['Open'].diff(12)\n",
    "df['4H_O']  = df['Open'].diff(12*4)\n",
    "df['1D_O']  = df['Open'].diff(12*24)\n",
    "df['1W_O']  = df['Open'].diff(12*24*5)\n",
    "df['1M_O']  = df['Open'].diff(12*24*5*4)\n",
    "\n",
    "df['15m_H'] = df['High'].diff(3)\n",
    "df['30m_H'] = df['High'].diff(6)\n",
    "df['1H_H']  = df['High'].diff(12)\n",
    "df['4H_H']  = df['High'].diff(12*4)\n",
    "df['1D_H']  = df['High'].diff(12*24)\n",
    "df['1W_H']  = df['High'].diff(12*24*5)\n",
    "df['1M_H']  = df['High'].diff(12*24*5*4)\n",
    "\n",
    "df['15m_L'] = df['Low'].diff(3)\n",
    "df['30m_L'] = df['Low'].diff(6)\n",
    "df['1H_L']  = df['Low'].diff(12)\n",
    "df['4H_L']  = df['Low'].diff(12*4)\n",
    "df['1D_L']  = df['Low'].diff(12*24)\n",
    "df['1W_L']  = df['Low'].diff(12*24*5)\n",
    "df['1M_L']  = df['Low'].diff(12*24*5*4)\n",
    "\n",
    "df['15m_C'] = df['Close'].diff(3)\n",
    "df['30m_C'] = df['Close'].diff(6)\n",
    "df['1H_C'] = df['Close'].diff(12)\n",
    "df['4H_C'] = df['Close'].diff(12*4)\n",
    "df['1D_C'] = df['Close'].diff(12*24)\n",
    "df['1W_C'] = df['Close'].diff(12*24*5)\n",
    "df['1M_C'] = df['Close'].diff(12*24*5*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['15m_O_Log'] = Log_Diff(df['Open'],3)\n",
    "df['30m_O_Log'] = Log_Diff(df['Open'],6)\n",
    "df['1H_O_Log']  = Log_Diff(df['Open'],12)\n",
    "df['4H_O_Log']  = Log_Diff(df['Open'],12*4)\n",
    "df['1D_O_Log']  = Log_Diff(df['Open'],12*24)\n",
    "df['1W_O_Log']  = Log_Diff(df['Open'],12*24*5)\n",
    "df['1M_O_Log']  = Log_Diff(df['Open'],12*24*5*4)\n",
    "\n",
    "df['30m_H_Log'] = Log_Diff(df['High'],6)\n",
    "df['1H_H_Log']  = Log_Diff(df['High'],12)\n",
    "df['4H_H_Log']  = Log_Diff(df['High'],12*4)\n",
    "df['1D_H_Log']  = Log_Diff(df['High'],12*24)\n",
    "df['1W_H_Log']  = Log_Diff(df['High'],12*24*5)\n",
    "df['1M_H_Log']  = Log_Diff(df['High'],12*24*5*4)\n",
    "\n",
    "df['15m_L_Log'] = Log_Diff(df['Low'],3)\n",
    "df['30m_L_Log'] = Log_Diff(df['Low'],6)\n",
    "df['1H_L_Log']  = Log_Diff(df['Low'],12)\n",
    "df['4H_L_Log']  = Log_Diff(df['Low'],12*4)\n",
    "df['1D_L_Log']  = Log_Diff(df['Low'],12*24)\n",
    "df['1W_L_Log']  = Log_Diff(df['Low'],12*24*5)\n",
    "df['1M_L_Log']  = Log_Diff(df['Low'],12*24*5*4)\n",
    "\n",
    "df['15m_C_Log'] = Log_Diff(df['Close'],3)\n",
    "df['30m_C_Log'] = Log_Diff(df['Close'],6)\n",
    "df['1H_C_Log']  = Log_Diff(df['Close'],12)\n",
    "df['4H_C_Log']  = Log_Diff(df['Close'],12*4)\n",
    "df['1D_C_Log']  = Log_Diff(df['Close'],12*24)\n",
    "df['1W_C_Log']  = Log_Diff(df['Close'],12*24*5)\n",
    "df['1M_C_Log']  = Log_Diff(df['Close'],12*24*5*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日付"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "# 曜日(0=日曜日、1=月曜日)を数値で取得\n",
    "df['Day of The Week'] = df['Date'].dt.dayofweek\n",
    "df['Hour'] = df['Date'].dt.hour\n",
    "#df['Minute'] = df['Date'].dt.minute\n",
    "df['Week'] = df['Date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['5m_mae'] = df['Close'].shift(1)\n",
    "df['15m_mae']= df['Close'].shift(3)\n",
    "df['30m_mae']= df['Close'].shift(6)\n",
    "df['1H_mae'] = df['Close'].shift(12)\n",
    "df['4H_mae'] = df['Close'].shift(12*4)\n",
    "df['1D_mae'] = df['Close'].shift(12*24)\n",
    "df['1W_mae'] = df['Close'].shift(12*24*5)\n",
    "df['1M_mae'] = df['Close'].shift(12*24*5*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テクニカル指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Technical_Index(object):\n",
    "    \"\"\"\n",
    "    テクニカル指標のクラス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    # 単純移動平均(SMA: Simple Moving Average)\n",
    "    def get_SMA(price_arr, period):\n",
    "        print('SMA:',period)\n",
    "        data = price_arr.values.tolist()\n",
    "        SMA = sma(data, period)\n",
    "        return SMA\n",
    "    \n",
    "    # 指数移動平均(EMA: Exponential Moving Average)\n",
    "    def get_EMA(price_arr, period):\n",
    "        print('EMA:',period)\n",
    "        data = price_arr.values.tolist()\n",
    "        EMA = ema(data, period)\n",
    "        return EMA \n",
    "    \n",
    "    # 指数移動平均(EMA: Exponential Moving Average)\n",
    "    def get_MACD(price_arr, long_period, short_period):\n",
    "        print('MACD:',long_period, short_period)\n",
    "        data = price_arr.values.tolist()\n",
    "        MACD = macd(data, long_period, short_period)\n",
    "        return MACD \n",
    "    \n",
    "    # ボリンジャーバンド(BB: Bollinger Bands)\n",
    "    def get_BB(price_arr, period):\n",
    "        print('BB:',period)\n",
    "        mean = price_arr.rolling(period).mean()\n",
    "        std  = price_arr.rolling(period).std()\n",
    "        upp1  = mean + (std * 1)\n",
    "        low1  = mean - (std * 1)\n",
    "        upp2  = mean + (std * 2)\n",
    "        low2  = mean - (std * 2)\n",
    "        upp3  = mean + (std * 3)\n",
    "        low3  = mean - (std * 3)\n",
    "        return(mean, std, upp1, low1, upp2, low2, upp3, low3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMA: 200\n",
      "SMA: 300\n",
      "EMA: 150\n"
     ]
    }
   ],
   "source": [
    "#SMAを計算\n",
    "sma_200 = Technical_Index.get_SMA(df['Close'], period=200)\n",
    "df['SMA_200'] = sma_200\n",
    "\n",
    "sma_300 = Technical_Index.get_SMA(df['Close'], period=300)\n",
    "df['SMA_300'] = sma_300\n",
    "\n",
    "\n",
    "#EMAを計算\n",
    "ema_150 = Technical_Index.get_EMA(df['Close'], period=150)\n",
    "df['EMA_150'] = ema_150\n",
    "\n",
    "ema_250 = Technical_Index.get_EMA(df['Close'], period=250)\n",
    "df['EMA_250'] = ema_250\n",
    "\n",
    "#SMAを計算\n",
    "sma_25 = Technical_Index.get_SMA(df['Close'], period=25)\n",
    "df['SMA_25'] = sma_25\n",
    "\n",
    "#EMAを計算\n",
    "ema_10 = Technical_Index.get_EMA(df['Close'], period=10)\n",
    "df['EMA_10'] = ema_10\n",
    "\n",
    "# MACDを計算\n",
    "_macd = Technical_Index.get_MACD(df['Close'], long_period=26, short_period=12)\n",
    "df['MACD'] = _macd\n",
    "\n",
    "# BolongerBandを計算\n",
    "BBand = Technical_Index.get_BB(df['Close'], period=25)\n",
    "df['BBand_Mead'] = BBand[0]\n",
    "df['BBand_Std']  = BBand[1]\n",
    "df['BB_Upp1']    = BBand[2]\n",
    "df['BB_Low1']    = BBand[3]\n",
    "df['BB_Upp2']    = BBand[4]\n",
    "df['BB_Low2']    = BBand[5]\n",
    "df['BB_Upp3']    = BBand[6]\n",
    "df['BB_Low3']    = BBand[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMAを計算\n",
    "sma_25 = Technical_Index.get_SMA(df['1D_mae'], period=25)\n",
    "df['1D_maeSMA_25'] = sma_25\n",
    "\n",
    "#EMAを計算\n",
    "ema_10 = Technical_Index.get_EMA(df['1D_mae'], period=10)\n",
    "df['1D_maeEMA_10'] = ema_10\n",
    "\n",
    "# MACDを計算\n",
    "_macd = Technical_Index.get_MACD(df['1D_mae'], long_period=26, short_period=12)\n",
    "df['1D_maeMACD'] = _macd\n",
    "\n",
    "# BolongerBandを計算\n",
    "BBand = Technical_Index.get_BB(df['1D_mae'], period=25)\n",
    "df['1D_maeBBand_Mead'] = BBand[0]\n",
    "df['1D_maeBBand_Std']  = BBand[1]\n",
    "df['1D_maeBB_Upp1']    = BBand[2]\n",
    "df['1D_maeBB_Low1']    = BBand[3]\n",
    "df['1D_maeBB_Upp2']    = BBand[4]\n",
    "df['1D_maeBB_Low2']    = BBand[5]\n",
    "df['1D_maeBB_Upp3']    = BBand[6]\n",
    "df['1D_maeBB_Low3']    = BBand[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 外部要因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other(_df, df_merge, instrument):\n",
    "    \n",
    "    print('start:',instrument)\n",
    "    \n",
    "    df_merge = pd.merge_asof(df_merge, _df, left_on='Date',\n",
    "                         right_on='Date', by='Date')\n",
    "\n",
    "    df_merge['{}_5m_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(1)\n",
    "    df_merge['{}_15m_mae'.format(instrument)] = df_merge['{}_Close'.format(instrument)].shift(3)\n",
    "    df_merge['{}_30m_mae'.format(instrument)] = df_merge['{}_Close'.format(instrument)].shift(6)\n",
    "    df_merge['{}_1H_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(12)\n",
    "    df_merge['{}_4H_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(12*4)\n",
    "    df_merge['{}_1D_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(12*24)\n",
    "    df_merge['{}_1W_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(12*24*5)\n",
    "    df_merge['{}_1M_mae'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].shift(12*24*5*4)\n",
    "    \n",
    "    df_merge['{}_15m_C'.format(instrument)] = df_merge['{}_Close'.format(instrument)].diff(3)\n",
    "    df_merge['{}_30m_C'.format(instrument)] = df_merge['{}_Close'.format(instrument)].diff(6)\n",
    "    df_merge['{}_1H_C'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].diff(12)\n",
    "    df_merge['{}_4H_C'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].diff(12*4)\n",
    "    df_merge['{}_1D_C'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].diff(12*24)\n",
    "    df_merge['{}_1W_C'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].diff(12*24*5)\n",
    "    df_merge['{}_1M_C'.format(instrument)]  = df_merge['{}_Close'.format(instrument)].diff(12*24*5*4)\n",
    "\n",
    "    df_merge['{}_15m_C_Log'.format(instrument)] = Log_Diff(df_merge['{}_Close'.format(instrument)],3)\n",
    "    df_merge['{}_30m_C_Log'.format(instrument)] = Log_Diff(df_merge['{}_Close'.format(instrument)],6)\n",
    "    df_merge['{}_1H_C_Log'.format(instrument)]  = Log_Diff(df_merge['{}_Close'.format(instrument)],12)\n",
    "    df_merge['{}_4H_C_Log'.format(instrument)]  = Log_Diff(df_merge['{}_Close'.format(instrument)],12*4)\n",
    "    df_merge['{}_1D_C_Log'.format(instrument)]  = Log_Diff(df_merge['{}_Close'.format(instrument)],12*24)\n",
    "    df_merge['{}_1W_C_Log'.format(instrument)]  = Log_Diff(df_merge['{}_Close'.format(instrument)],12*24*5)\n",
    "    df_merge['{}_1M_C_Log'.format(instrument)]  = Log_Diff(df_merge['{}_Close'.format(instrument)],12*24*5*4)\n",
    "    \n",
    "    #SMA\n",
    "    #df_merge['{}_SMA_25'.format(instrument)] = Technical_Index.get_SMA(df_merge['{}_Close'.format(instrument)], period=25)\n",
    "    #df_merge['{}_SMA_200'.format(instrument)] = Technical_Index.get_SMA(df_merge['{}_Close'.format(instrument)], period=200)\n",
    "    #df_merge['{}_SMA_300'.format(instrument)] = Technical_Index.get_SMA(df_merge['{}_Close'.format(instrument)], period=300)\n",
    "    \n",
    "    #EMA\n",
    "    #df_merge['{}_EMA_15'.format(instrument)] = Technical_Index.get_EMA(df_merge['{}_Close'.format(instrument)], period=15)\n",
    "    df_merge['{}_EMA_150'.format(instrument)] = Technical_Index.get_EMA(df_merge['{}_Close'.format(instrument)], period=150)\n",
    "    #df_merge['{}_EMA_250'.format(instrument)] = Technical_Index.get_EMA(df_merge['{}_Close'.format(instrument)], period=250)\n",
    "      \n",
    "    df_merge = df_merge.interpolate()\n",
    "    \n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df_EURJPY['Date'].dtype)\n",
    "print(df['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = get_other(df_EURJPY, df, 'EUR_JPY')\n",
    "df = get_other(df_GBPJPY, df, 'GBP_JPY')\n",
    "df = get_other(df_GBPUSD, df, 'GBP_USD')\n",
    "df = get_other(df_EURUSD, df, 'EUR_USD')\n",
    "df = get_other(df_SGDJPY, df, 'SGD_JPY')\n",
    "df = get_other(df_JP225,  df, 'JP225_USD')\n",
    "df = get_other(df_US30,   df, 'US30_USD')\n",
    "df = get_other(df_NAS100, df, 'NAS100_USD')\n",
    "df = get_other(df_SPX500, df, 'SPX500_USD')\n",
    "df = get_other(df_EU50,   df, 'EU50_EUR')\n",
    "df = get_other(df_DE30,   df, 'DE30_EUR')\n",
    "df = get_other(df_AU200,  df, 'AU200_AUD')\n",
    "df = get_other(df_USB05Y, df, 'USB05Y_USD')\n",
    "df = get_other(df_USB10Y, df, 'USB10Y_USD')\n",
    "df = get_other(df_XAUEUR, df, 'XAU_EUR')\n",
    "df = get_other(df_XAGGBP, df, 'XAG_GBP')\n",
    "df = get_other(df_XAUXAG, df, 'XAU_XAG')\n",
    "df = get_other(df_WHEAT,  df, 'WHEAT_USD')\n",
    "\n",
    "df = df.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_EURJPY\n",
    "del df_GBPJPY\n",
    "del df_GBPUSD\n",
    "del df_EURUSD\n",
    "del df_SGDJPY\n",
    "del df_JP225\n",
    "del df_US30\n",
    "del df_NAS100\n",
    "del df_SPX500\n",
    "del df_EU50\n",
    "del df_DE30\n",
    "del df_AU200\n",
    "del df_USB05Y\n",
    "del df_USB10Y\n",
    "del df_XAUEUR\n",
    "del df_XAGGBP\n",
    "del df_XAUXAG\n",
    "del df_WHEAT\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['Date'] > '2016-01-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2018/6/01 00:00:00'\n",
    "train, test = df1[df1['Date'] < split_date], df1[df1['Date']>=split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "XX = [ 'Open', 'High', 'Low', 'Close', 'Volume', '15m_O', '30m_O', '1H_O',\n",
    "       '4H_O', '1D_O', '1W_O', '1M_O', '15m_H', '30m_H', '1H_H', '4H_H',\n",
    "       '1D_H', '1W_H', '1M_H', '15m_L', '30m_L', '1H_L', '4H_L', '1D_L',\n",
    "       '1W_L', '1M_L', '15m_C', '30m_C', '1H_C', '4H_C', '1D_C', '1W_C',\n",
    "       '1M_C', '15m_O_Log', '30m_O_Log', '1H_O_Log', '4H_O_Log',\n",
    "       '1D_O_Log', '1W_O_Log', '1M_O_Log', '30m_H_Log', '1H_H_Log',\n",
    "       '4H_H_Log', '1D_H_Log', '1W_H_Log', '1M_H_Log', '15m_L_Log',\n",
    "       '30m_L_Log', '1H_L_Log', '4H_L_Log', '1D_L_Log', '1W_L_Log',\n",
    "       '1M_L_Log', '15m_C_Log', '30m_C_Log', '1H_C_Log', '4H_C_Log',\n",
    "       '1D_C_Log', '1W_C_Log', '1M_C_Log', 'Year', 'Month', 'Day',\n",
    "       'Day of The Week', 'Hour', 'Week', '5m_mae', '15m_mae', '30m_mae',\n",
    "       '1H_mae', '4H_mae', '1D_mae', '1W_mae', '1M_mae', 'SMA_200',\n",
    "       'SMA_300', 'EMA_150', 'EMA_250', 'SMA_25', 'EMA_10', 'MACD',\n",
    "       'BBand_Mead', 'BBand_Std', 'BB_Upp1', 'BB_Low1', 'BB_Upp2',\n",
    "       'BB_Low2', 'BB_Upp3', 'BB_Low3', '1D_maeSMA_25', '1D_maeEMA_10',\n",
    "       '1D_maeMACD', '1D_maeBBand_Mead', '1D_maeBBand_Std',\n",
    "       '1D_maeBB_Upp1', '1D_maeBB_Low1', '1D_maeBB_Upp2', '1D_maeBB_Low2',\n",
    "       '1D_maeBB_Upp3', '1D_maeBB_Low3']\n",
    "\n",
    "standardscaler = StandardScaler()\n",
    "X_train = standardscaler.fit_transform(train.loc[:,XX])\n",
    "X_test = standardscaler.fit_transform(test.loc[:,XX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train.loc[:,['PROFIT_2H']],int)\n",
    "y_test  = np.array(test.loc[:,['PROFIT_2H']],int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=test.loc[:,XX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179830, 586)\n",
      "float64\n",
      "(179830, 1)\n",
      "int32\n",
      "(61466, 586)\n",
      "(61466, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.dtype)\n",
    "print(y_train.shape)\n",
    "print(y_train.dtype)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               300544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 465,921\n",
      "Trainable params: 465,409\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#モデルの定義\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import LeakyReLU, Dense, Dropout, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.initializers import he_normal,lecun_normal,he_uniform\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train.shape[1],), kernel_initializer=he_uniform()))\n",
    "model.add(layers.LeakyReLU(alpha=0.4))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,kernel_regularizer=regularizers.l2(1.)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.LeakyReLU(alpha=0.4))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.LeakyReLU(alpha=0.4))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss ='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.hdf5\".format('Dense_2H_1')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_acc', verbose=1,\n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.8, patience=5,\n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\goomo\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 179830 samples, validate on 61466 samples\n",
      "Epoch 1/100\n",
      "179830/179830 [==============================] - 8s 45us/step - loss: 13.0818 - acc: 0.5097 - val_loss: 0.7848 - val_acc: 0.5115\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.51149, saving model to Dense_2H_1_weights.hdf5\n",
      "Epoch 2/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7450 - acc: 0.5234 - val_loss: 0.7168 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.51149 to 0.53226, saving model to Dense_2H_1_weights.hdf5\n",
      "Epoch 3/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7101 - acc: 0.5312 - val_loss: 0.7093 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53226 to 0.53273, saving model to Dense_2H_1_weights.hdf5\n",
      "Epoch 4/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7060 - acc: 0.5358 - val_loss: 0.7068 - val_acc: 0.5142\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.53273\n",
      "Epoch 5/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7068 - acc: 0.5366 - val_loss: 0.7079 - val_acc: 0.5287\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.53273\n",
      "Epoch 6/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7099 - acc: 0.5362 - val_loss: 0.7230 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.53273\n",
      "Epoch 7/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7118 - acc: 0.5359 - val_loss: 0.7185 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.53273\n",
      "Epoch 8/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7150 - acc: 0.5372 - val_loss: 0.7208 - val_acc: 0.5142\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.53273\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 9/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7125 - acc: 0.5407 - val_loss: 0.7137 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.53273 to 0.53548, saving model to Dense_2H_1_weights.hdf5\n",
      "Epoch 10/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7131 - acc: 0.5420 - val_loss: 0.7255 - val_acc: 0.5279\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.53548\n",
      "Epoch 11/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7134 - acc: 0.5436 - val_loss: 0.7189 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.53548\n",
      "Epoch 12/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7137 - acc: 0.5455 - val_loss: 0.7180 - val_acc: 0.5387\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.53548 to 0.53870, saving model to Dense_2H_1_weights.hdf5\n",
      "Epoch 13/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7128 - acc: 0.5477 - val_loss: 0.7171 - val_acc: 0.5358\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.53870\n",
      "Epoch 14/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7129 - acc: 0.5497 - val_loss: 0.7117 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.53870\n",
      "Epoch 15/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7128 - acc: 0.5500 - val_loss: 0.7221 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.53870\n",
      "Epoch 16/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7120 - acc: 0.5521 - val_loss: 0.7263 - val_acc: 0.5181\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.53870\n",
      "Epoch 17/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7111 - acc: 0.5539 - val_loss: 0.7236 - val_acc: 0.5284\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.53870\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 18/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7058 - acc: 0.5557 - val_loss: 0.7208 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.53870\n",
      "Epoch 19/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7056 - acc: 0.5570 - val_loss: 0.7245 - val_acc: 0.5242\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.53870\n",
      "Epoch 20/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7067 - acc: 0.5575 - val_loss: 0.7150 - val_acc: 0.5291\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.53870\n",
      "Epoch 21/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7052 - acc: 0.5601 - val_loss: 0.7166 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.53870\n",
      "Epoch 22/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7051 - acc: 0.5591 - val_loss: 0.7269 - val_acc: 0.5226\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.53870\n",
      "Epoch 23/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.7054 - acc: 0.5607 - val_loss: 0.7427 - val_acc: 0.5259\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.53870\n",
      "Epoch 24/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.7045 - acc: 0.5626 - val_loss: 0.7222 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.53870\n",
      "Epoch 25/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.7039 - acc: 0.5642 - val_loss: 0.7156 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.53870\n",
      "Epoch 26/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.7025 - acc: 0.5642 - val_loss: 0.7320 - val_acc: 0.5274\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.53870\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 27/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.6998 - acc: 0.5684 - val_loss: 0.7440 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.53870\n",
      "Epoch 28/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.6987 - acc: 0.5706 - val_loss: 0.7353 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.53870\n",
      "Epoch 29/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6980 - acc: 0.5714 - val_loss: 0.7503 - val_acc: 0.5213\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.53870\n",
      "Epoch 30/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6980 - acc: 0.5737 - val_loss: 0.7403 - val_acc: 0.5181\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.53870\n",
      "Epoch 31/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6976 - acc: 0.5751 - val_loss: 0.7536 - val_acc: 0.5253\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.53870\n",
      "Epoch 32/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6967 - acc: 0.5757 - val_loss: 0.7484 - val_acc: 0.5203\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.53870\n",
      "Epoch 33/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6969 - acc: 0.5782 - val_loss: 0.7607 - val_acc: 0.5183\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.53870\n",
      "Epoch 34/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6940 - acc: 0.5805 - val_loss: 0.7740 - val_acc: 0.5171\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.53870\n",
      "Epoch 35/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.6949 - acc: 0.5820 - val_loss: 0.7850 - val_acc: 0.5140\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.53870\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "Epoch 36/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.6903 - acc: 0.5861 - val_loss: 0.7782 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.53870\n",
      "Epoch 37/100\n",
      "179830/179830 [==============================] - 5s 28us/step - loss: 0.6898 - acc: 0.5883 - val_loss: 0.7787 - val_acc: 0.5181\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.53870\n",
      "Epoch 38/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6895 - acc: 0.5906 - val_loss: 0.8161 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.53870\n",
      "Epoch 39/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6880 - acc: 0.5923 - val_loss: 0.8413 - val_acc: 0.5136\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.53870\n",
      "Epoch 40/100\n",
      "179830/179830 [==============================] - 5s 29us/step - loss: 0.6880 - acc: 0.5935 - val_loss: 0.8523 - val_acc: 0.5107\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.53870\n",
      "Epoch 41/100\n",
      " 95488/179830 [==============>...............] - ETA: 2s - loss: 0.6866 - acc: 0.5956"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-f1c20d61b20d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m           callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2653\u001b[0m                 array_vals.append(\n\u001b[0;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[1;32m-> 2655\u001b[1;33m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "#モデルをサイレントモードで適合\n",
    "history = model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=num_epochs, batch_size=batch_size, verbose=1,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61466/61466 [==============================] - 2s 27us/step\n",
      "[0.795475702083212, 0.5366706797273161]\n"
     ]
    }
   ],
   "source": [
    "#from keras.models import load_weights\n",
    "model.load_weights('Dense_2H_1_weights.hdf5')\n",
    "X_predict = model.predict(X_test)\n",
    "X_evaluate = model.evaluate(X_test,y_test)\n",
    "print(X_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179795, 36, 586)\n"
     ]
    }
   ],
   "source": [
    "window_len = 36        # 入力系列数\n",
    "n_in = X_train.shape[1]   # 学習データ（＝入力）の列数\n",
    "len_seq = X_train.shape[0] - window_len + 1\n",
    "data = []\n",
    "\n",
    "for i in range(0, len_seq):\n",
    "    data.append(X_train[i:i+window_len, :])\n",
    "x = np.array(data).reshape(len(data), window_len, n_in)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61431, 36, 586)\n"
     ]
    }
   ],
   "source": [
    "window_len = 36            # 入力系列数\n",
    "n_in = X_test.shape[1]   # 学習データ（＝入力）の列数\n",
    "len_seq = X_test.shape[0] - window_len + 1\n",
    "target = []\n",
    "\n",
    "for i in range(0, len_seq):\n",
    "    target.append(X_test[i:i+window_len, :])\n",
    "t = np.array(target).reshape(len(target), window_len, n_in)\n",
    "\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train = y_train[window_len -1 :x.shape[0] + window_len +1]\n",
    "yy_test = y_test[window_len -1 :t.shape[0] + window_len +1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 36, 586)           2344      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 36, 36)            89712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 36, 36)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 36)                10512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 19        \n",
      "=================================================================\n",
      "Total params: 103,253\n",
      "Trainable params: 102,081\n",
      "Non-trainable params: 1,172\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import LeakyReLU, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(BatchNormalization(input_shape=(x.shape[1], x.shape[2])))\n",
    "#model.add(layers.GRU(32,activation='relu',\n",
    "model.add(layers.LSTM(36,input_shape=(x.shape[1], x.shape[2]),kernel_regularizer=regularizers.l2(0.01),\n",
    "                     dropout = 0.3, recurrent_dropout= 0.3, return_sequences = True))\n",
    "model.add(layers.LeakyReLU(alpha=0.3))\n",
    "#model.add(layers.GRU(32,kernel_regularizer=regularizers.l2(0.01),dropout = 0.3, recurrent_dropout= 0.3,return_sequences = False))\n",
    "#model.add(layers.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.LSTM(36,input_shape=(x.shape[1], x.shape[2]),kernel_regularizer=regularizers.l2(0.01),\n",
    "                     dropout = 0.3, recurrent_dropout= 0.3, return_sequences = False))\n",
    "model.add(layers.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Dense(18,kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(layers.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('lstm_2h')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_acc', verbose=1,\n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.8, patience=7,\n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179795 samples, validate on 61431 samples\n",
      "Epoch 1/30\n",
      "179795/179795 [==============================] - 313s 2ms/step - loss: 1.0261 - acc: 0.5151 - val_loss: 0.6991 - val_acc: 0.5118\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.51181, saving model to lstm_2h_weights.best.hdf5\n",
      "Epoch 2/30\n",
      "179795/179795 [==============================] - 290s 2ms/step - loss: 0.6951 - acc: 0.5151 - val_loss: 0.6936 - val_acc: 0.5107\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.51181\n",
      "Epoch 3/30\n",
      " 30720/179795 [====>.........................] - ETA: 3:31 - loss: 0.6936 - acc: 0.5143"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-c2b4654e3411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m           callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "\n",
    "#モデルをサイレントモードで適合\n",
    "history = model.fit(x, yy_train,\n",
    "          validation_data=(t, yy_test),\n",
    "          epochs=num_epochs, batch_size=batch_size, verbose=1,shuffle=True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_weights\n",
    "model.load_weights('lstm_2h_weights.hdf5')\n",
    "X_predict = model.predict(t)\n",
    "X_evaluate = model.evaluate(t,yy_test)\n",
    "print(X_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 36, 586)           2344      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 128)           375168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 28, 256)           164096    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 26, 512)           393728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 26, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 26, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 26, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 26, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 26, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 26, 256)           131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 26, 256)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,807,913\n",
      "Trainable params: 5,806,741\n",
      "Non-trainable params: 1,172\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed, GlobalAveragePooling1D\n",
    "from keras import regularizers\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape=(x.shape[1], x.shape[2])))\n",
    "stroke_read_model.add(Conv1D(128, (5,)))\n",
    "stroke_read_model.add(LeakyReLU(alpha=0.3))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (5,)))\n",
    "stroke_read_model.add(LeakyReLU(alpha=0.3))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(512, (3,)))\n",
    "stroke_read_model.add(LeakyReLU(alpha=0.3))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Bidirectional(LSTM(256, dropout = 0.3, recurrent_dropout= 0.3,  return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(256, dropout = 0.3, recurrent_dropout= 0.3, return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(256, dropout = 0.3, kernel_regularizer=regularizers.l2(0.001),\n",
    "                                         recurrent_dropout= 0.3, return_sequences = True)))\n",
    "stroke_read_model.add(TimeDistributed(Dense(256)))\n",
    "stroke_read_model.add(LeakyReLU(alpha=0.3))\n",
    "stroke_read_model.add(GlobalAveragePooling1D())\n",
    "stroke_read_model.add(Dense(64,kernel_regularizer=regularizers.l2(0.01)))\n",
    "stroke_read_model.add(LeakyReLU(alpha=0.3))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Dense(1, activation = 'sigmoid'))\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'binary_crossentropy', \n",
    "                          metrics = ['accuracy'])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('lstm_bid_time2h')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.8, patience=5, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179795 samples, validate on 61431 samples\n",
      "Epoch 1/30\n",
      " 30208/179795 [====>.........................] - ETA: 4:32 - loss: 1.7651 - acc: 0.5061"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-89ba9c07ad15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = stroke_read_model.fit(x, yy_train,\n",
    "          validation_data=(t, yy_test),\n",
    "          epochs=num_epochs, batch_size=512, shuffle= True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61417/61417 [==============================] - 168s 3ms/step\n",
      "[0.6941858820172835, 0.5208004298488985]\n"
     ]
    }
   ],
   "source": [
    "stroke_read_model.load_weights('lstm_bid_time2h_weights.best.hdf5')\n",
    "X_predict =stroke_read_model.predict(t)\n",
    "X_evaluate = stroke_read_model.evaluate(t,yy_test)\n",
    "print(X_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
